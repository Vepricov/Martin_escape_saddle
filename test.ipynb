{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2e12e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n",
    "                  [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "a.transpose_(0, 1)\n",
    "a = a.contiguous()\n",
    "a = a.view(2, -1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46f18267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "view(*shape) -> Tensor\n",
      "\n",
      "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "different :attr:`shape`.\n",
      "\n",
      "The returned tensor shares the same data and must have the same number\n",
      "of elements, but may have a different size. For a tensor to be viewed, the new\n",
      "view size must be compatible with its original size and stride, i.e., each new\n",
      "view dimension must either be a subspace of an original dimension, or only span\n",
      "across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      "contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      "\n",
      ".. math::\n",
      "\n",
      "  \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      "\n",
      "Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      "without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      ":meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      "returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      ":meth:`contiguous`) otherwise.\n",
      "\n",
      "Args:\n",
      "    shape (torch.Size or int...): the desired size\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(4, 4)\n",
      "    >>> x.size()\n",
      "    torch.Size([4, 4])\n",
      "    >>> y = x.view(16)\n",
      "    >>> y.size()\n",
      "    torch.Size([16])\n",
      "    >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      "    >>> z.size()\n",
      "    torch.Size([2, 8])\n",
      "\n",
      "    >>> a = torch.randn(1, 2, 3, 4)\n",
      "    >>> a.size()\n",
      "    torch.Size([1, 2, 3, 4])\n",
      "    >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      "    >>> b.size()\n",
      "    torch.Size([1, 3, 2, 4])\n",
      "    >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      "    >>> c.size()\n",
      "    torch.Size([1, 3, 2, 4])\n",
      "    >>> torch.equal(b, c)\n",
      "    False\n",
      "\n",
      "\n",
      ".. method:: view(dtype) -> Tensor\n",
      "   :noindex:\n",
      "\n",
      "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "different :attr:`dtype`.\n",
      "\n",
      "If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
      "then the size of the last dimension of the output will be scaled\n",
      "proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
      "``self.dtype``, then each pair of elements in the last dimension of\n",
      ":attr:`self` will be combined, and the size of the last dimension of the output\n",
      "will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
      "of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
      "be split in two, and the size of the last dimension of the output will be\n",
      "double that of :attr:`self`. For this to be possible, the following conditions\n",
      "must be true:\n",
      "\n",
      "    * ``self.dim()`` must be greater than 0.\n",
      "    * ``self.stride(-1)`` must be 1.\n",
      "\n",
      "Additionally, if the element size of :attr:`dtype` is greater than that of\n",
      "``self.dtype``, the following conditions must be true as well:\n",
      "\n",
      "    * ``self.size(-1)`` must be divisible by the ratio between the element\n",
      "      sizes of the dtypes.\n",
      "    * ``self.storage_offset()`` must be divisible by the ratio between the\n",
      "      element sizes of the dtypes.\n",
      "    * The strides of all dimensions, except the last dimension, must be\n",
      "      divisible by the ratio between the element sizes of the dtypes.\n",
      "\n",
      "If any of the above conditions are not met, an error is thrown.\n",
      "\n",
      ".. warning::\n",
      "\n",
      "    This overload is not supported by TorchScript, and using it in a Torchscript\n",
      "    program will cause undefined behavior.\n",
      "\n",
      "\n",
      "Args:\n",
      "    dtype (:class:`torch.dtype`): the desired dtype\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> x = torch.randn(4, 4)\n",
      "    >>> x\n",
      "    tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      "            [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "            [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "            [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "    >>> x.dtype\n",
      "    torch.float32\n",
      "\n",
      "    >>> y = x.view(torch.int32)\n",
      "    >>> y\n",
      "    tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      "            [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      "            [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      "            [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      "        dtype=torch.int32)\n",
      "    >>> y[0, 0] = 1000000000\n",
      "    >>> x\n",
      "    tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      "            [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      "            [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      "            [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      "\n",
      "    >>> x.view(torch.cfloat)\n",
      "    tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
      "            [-0.1520+0.7472j,  0.5617-0.8649j],\n",
      "            [-2.4724-0.0334j, -0.2976-0.8499j],\n",
      "            [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
      "    >>> x.view(torch.cfloat).size()\n",
      "    torch.Size([4, 2])\n",
      "\n",
      "    >>> x.view(torch.uint8)\n",
      "    tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
      "               8, 191],\n",
      "            [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
      "              93, 191],\n",
      "            [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
      "              89, 191],\n",
      "            [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
      "              28, 191]], dtype=torch.uint8)\n",
      "    >>> x.view(torch.uint8).size()\n",
      "    torch.Size([4, 16])\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "?a.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed926c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e520353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        ArgumentParser\n",
      "\u001b[0;31mString form:\u001b[0m ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n",
      "\u001b[0;31mFile:\u001b[0m        ~/miniconda3/lib/python3.10/argparse.py\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Object for parsing command line strings into Python objects.\n",
      "\n",
      "Keyword Arguments:\n",
      "    - prog -- The name of the program (default:\n",
      "        ``os.path.basename(sys.argv[0])``)\n",
      "    - usage -- A usage message (default: auto-generated from arguments)\n",
      "    - description -- A description of what the program does\n",
      "    - epilog -- Text following the argument descriptions\n",
      "    - parents -- Parsers whose arguments should be copied into this one\n",
      "    - formatter_class -- HelpFormatter class for printing help messages\n",
      "    - prefix_chars -- Characters that prefix optional arguments\n",
      "    - fromfile_prefix_chars -- Characters that prefix files containing\n",
      "        additional arguments\n",
      "    - argument_default -- The default value for all arguments\n",
      "    - conflict_handler -- String indicating how to handle conflicts\n",
      "    - add_help -- Add a -h/-help option\n",
      "    - allow_abbrev -- Allow long options to be abbreviated unambiguously\n",
      "    - exit_on_error -- Determines whether or not ArgumentParser exits with\n",
      "        error info when an error occurs"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"kind\", type=str,\n",
    "                    help=\"Kind of experiment: single-model, hidden-loop or hidden-sample\")\n",
    "?parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b10b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn.datasets` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file, make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e913e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel(\n",
      "  (linear): Linear(in_features=3, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim=3, output_dim=1):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim, dtype=X.dtype, bias=False)\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "model = LinearRegressionModel(input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "print(model)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "565b190b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3128, 0.3760, 0.1549],\n",
       "        [0.3760, 0.5122, 0.2396],\n",
       "        [0.1549, 0.2396, 0.1242]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, L, _ = torch.svd(model.linear.weight.T @ model.linear.weight)\n",
    "Q @ torch.diag(L) @ Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9ff6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3128, 0.3760, 0.1549],\n",
       "        [0.3760, 0.5122, 0.2396],\n",
       "        [0.1549, 0.2396, 0.1242]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight.T @ model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import soap\n",
    "import new_soap\n",
    "reload(soap)\n",
    "reload(new_soap)\n",
    "\n",
    "model = LinearRegressionModel(input_dim=X.shape[1], output_dim=y.shape[1])\n",
    "# optimizer = soap.SOAP(model.parameters(), \n",
    "#                  lr=3e-3, \n",
    "#                  betas=(.95, .95), \n",
    "#                  weight_decay=.01, \n",
    "#                  precondition_frequency=10)\n",
    "optimizer = new_soap.NEW_SOAP(model.parameters(), \n",
    "                 lr=3e-3, \n",
    "                 beta=.95, \n",
    "                 weight_decay=.01, \n",
    "                 precondition_frequency=10)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af7dfcb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m num_verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: loss \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, loss\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Papers/Martin_escape_saddle_point/zalupa.py:129\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m if \"exp_avg\" not in state:\n\u001b[1;32m    125\u001b[0m     # Exponential moving average of gradient values\n\u001b[1;32m    126\u001b[0m     state[\"exp_avg\"] = torch.zeros_like(grad)\n\u001b[1;32m    128\u001b[0m # if 'Q' not in state:\n\u001b[0;32m--> 129\u001b[0m #     self.init_preconditioner(\n\u001b[1;32m    130\u001b[0m #         grad,\n\u001b[1;32m    131\u001b[0m #         state,\n\u001b[1;32m    132\u001b[0m #         precondition_frequency=group['precondition_frequency'],\n\u001b[1;32m    133\u001b[0m #         precondition_1d=group['precondition_1d'],\n\u001b[1;32m    134\u001b[0m #         max_precond_dim=group['max_precond_dim'],\n\u001b[1;32m    135\u001b[0m #         merge_dims=group[\"merge_dims\"],\n\u001b[1;32m    136\u001b[0m #     )\n\u001b[1;32m    137\u001b[0m #     self.update_preconditioner(grad, state,\n\u001b[1;32m    138\u001b[0m #                                max_precond_dim=group['max_precond_dim'],\n\u001b[1;32m    139\u001b[0m #                                merge_dims=group[\"merge_dims\"],\n\u001b[1;32m    140\u001b[0m #                                precondition_1d=group[\"precondition_1d\"])\n\u001b[1;32m    141\u001b[0m #     continue # first step is skipped so that we never use the current gradients in the projection.\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m # Projecting gradients to the eigenbases of Shampoo's preconditioner \n\u001b[1;32m    144\u001b[0m # i.e. projecting to the eigenbases of matrices in state['GG']\n\u001b[1;32m    145\u001b[0m # print(\"$\"*60, \"\\ngrad.T@ grad and grad@ grad.T:\")\n\u001b[1;32m    146\u001b[0m # print(grad@grad.T)\n\u001b[1;32m    147\u001b[0m # print(grad.T@grad)\n\u001b[1;32m    148\u001b[0m # print(\"$\"*60, \"\\nstate[GG]:\")\n\u001b[1;32m    149\u001b[0m # for mat in state['GG']:\n\u001b[1;32m    150\u001b[0m #     print(mat)\n\u001b[1;32m    151\u001b[0m # print(\"$\"*60, \"\\nstate[Q]:\")\n\u001b[1;32m    152\u001b[0m # for mat in state['Q']:\n\u001b[1;32m    153\u001b[0m #     print(mat, \"\\n\", mat.T @ mat)\n\u001b[1;32m    154\u001b[0m grad_projected = self.project(grad, state, merge_dims=group[\"merge_dims\"], \n\u001b[1;32m    155\u001b[0m                               max_precond_dim=group['max_precond_dim'])\n\u001b[1;32m    157\u001b[0m # Decay the first and second moment running average coefficient\n\u001b[1;32m    158\u001b[0m # In-place operations to update the averages at the same time\n",
      "\u001b[0;31mKeyError\u001b[0m: 'beta'"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_verbose = 1\n",
    "for epoch in range(num_epochs):\n",
    "    pred_y = model(X)\n",
    " \n",
    "    loss = criterion(pred_y, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % num_verbose == 0:\n",
    "        print('Epoch {}: loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75508366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "martin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
